# Comparative Analysis of Seq2Seq and BART for Abstractive Text Summarization

This project involves a detailed comparative analysis of two advanced machine learning models, Seq2Seq and BART, used for the task of abstractive text summarization. The aim is to evaluate and compare the effectiveness of each model in generating concise and contextually accurate summaries of text.

## Project Description

Abstractive text summarization is a challenging NLP task that requires models to understand and then paraphrase the content. This project compares two models:
- **Seq2Seq**: Utilizes LSTM networks, known for their efficacy in sequence prediction but requiring extensive training times.
- **BART**: Leverages a transformer-based architecture with robust pre-training, offering improved accuracy and efficiency in text summarization.

Our analysis is backed by quantitative assessments using ROUGE scores and qualitative evaluations through human reviews, focusing on the models' ability to generate coherent and relevant summaries.
